{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os \n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download = True, transform = transforms.ToTensor() )\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download = True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = trainset.train_data.float() / 255\n",
    "train_label = trainset.train_labels\n",
    "test_data = testset.test_data.float() / 255\n",
    "test_label = testset.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_data = torch.cat([train_data, test_data], dim=0)\n",
    "total_label = torch.cat([train_label, test_label], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = np.compress(np.in1d(total_label,[1,2,3,4,5,6,7,8,9]), \n",
    "                          total_data, axis=0)\n",
    "normal_label = np.compress(np.in1d(total_label,[1,2,3,4,5,6,7,8,9]), \n",
    "                           total_label, axis=0)\n",
    "novel_data = np.compress(np.in1d(total_label,[0]),\n",
    "                        total_data, axis=0)\n",
    "novel_label = np.compress(np.in1d(total_label,[0]), \n",
    "                         total_label, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(normal_data.shape[0]*0.6) # 60% train\n",
    "valid_size = int(normal_data.shape[0]*0.2) # 20% valid\n",
    "test_size = int(normal_data.shape[0]*0.2) # 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data[:train_size]\n",
    "valid_data = normal_data[train_size:train_size+valid_size]\n",
    "test_data = normal_data[train_size+valid_size:]\n",
    "train_label = normal_label[:train_size]\n",
    "valid_label = normal_label[train_size:train_size+valid_size]\n",
    "test_label = normal_label[train_size+valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37858, 28, 28])\n",
      "torch.Size([12619, 28, 28])\n",
      "torch.Size([12620, 28, 28])\n",
      "torch.Size([37858])\n",
      "torch.Size([12619])\n",
      "torch.Size([12620])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty_ratio = 0.35\n",
    "novelty_size = int(test_data.shape[0]/(1-novelty_ratio))\n",
    "test_data = torch.cat([test_data, novel_data[:novelty_size]], dim = 0)\n",
    "test_label = torch.cat([test_label, novel_label[:novelty_size]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19523, 28, 28])\n",
      "torch.Size([19523])\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.view(-1,28*28) # [x , 28 ,28] -> [x, 28*28]\n",
    "valid_data = valid_data.view(-1,28*28)\n",
    "test_data = test_data.view(-1,28*28)\n",
    "train_label = train_label.view(-1,1)\n",
    "valid_label = valid_label.view(-1,1)\n",
    "test_label = test_label.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37858, 784])\n",
      "torch.Size([12619, 784])\n",
      "torch.Size([19523, 784])\n",
      "torch.Size([37858, 1])\n",
      "torch.Size([12619, 1])\n",
      "torch.Size([19523, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_label.shape)\n",
    "print(valid_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [4],\n",
       "        [1],\n",
       "        ...,\n",
       "        [2],\n",
       "        [7],\n",
       "        [1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.cat([train_data, train_label.float()], dim=1)\n",
    "validset = torch.cat([valid_data, valid_label.float()], dim=1)\n",
    "testset = torch.cat([test_data, test_label.float()], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "trainset = DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
    "validset = DataLoader(valid_data, batch_size = 1000, shuffle=True)\n",
    "testset = DataLoader(testset, batch_size = 1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size = 784, h_dim = 400, z_dim = 20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu+eps*std\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return self.fc5(h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n"
     ]
    }
   ],
   "source": [
    "for x in trainset:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/30], Step [10/148], Reconst Loss: 14910.4912, KL Div: 1709.6306\n",
      "Epoch[1/30], Step [20/148], Reconst Loss: 14245.1777, KL Div: 1466.5925\n",
      "Epoch[1/30], Step [30/148], Reconst Loss: 13159.5215, KL Div: 1129.3564\n",
      "Epoch[1/30], Step [40/148], Reconst Loss: 11818.0088, KL Div: 1085.5063\n",
      "Epoch[1/30], Step [50/148], Reconst Loss: 11350.5244, KL Div: 1241.6716\n",
      "Epoch[1/30], Step [60/148], Reconst Loss: 10956.9346, KL Div: 1330.9694\n",
      "Epoch[1/30], Step [70/148], Reconst Loss: 9887.4453, KL Div: 1475.9283\n",
      "Epoch[1/30], Step [80/148], Reconst Loss: 9572.9170, KL Div: 1681.7631\n",
      "Epoch[1/30], Step [90/148], Reconst Loss: 9543.0254, KL Div: 1724.1128\n",
      "Epoch[1/30], Step [100/148], Reconst Loss: 9166.7451, KL Div: 1814.8325\n",
      "Epoch[1/30], Step [110/148], Reconst Loss: 8828.0654, KL Div: 1850.2240\n",
      "Epoch[1/30], Step [120/148], Reconst Loss: 8456.3506, KL Div: 1791.2833\n",
      "Epoch[1/30], Step [130/148], Reconst Loss: 8804.5576, KL Div: 1852.1927\n",
      "Epoch[1/30], Step [140/148], Reconst Loss: 8060.9761, KL Div: 1970.9828\n",
      "train_loss : 11000.35085317251 test_loss : 37867.4927734375\n",
      "Epoch[2/30], Step [10/148], Reconst Loss: 8169.5923, KL Div: 2106.0830\n",
      "Epoch[2/30], Step [20/148], Reconst Loss: 7665.9902, KL Div: 2062.2573\n",
      "Epoch[2/30], Step [30/148], Reconst Loss: 7703.0200, KL Div: 1958.5548\n",
      "Epoch[2/30], Step [40/148], Reconst Loss: 7575.8086, KL Div: 2153.1641\n",
      "Epoch[2/30], Step [50/148], Reconst Loss: 7168.8281, KL Div: 2098.3511\n",
      "Epoch[2/30], Step [60/148], Reconst Loss: 7569.3594, KL Div: 2135.3831\n",
      "Epoch[2/30], Step [70/148], Reconst Loss: 7227.5229, KL Div: 2224.4182\n",
      "Epoch[2/30], Step [80/148], Reconst Loss: 7228.8706, KL Div: 2183.0376\n",
      "Epoch[2/30], Step [90/148], Reconst Loss: 7137.5015, KL Div: 2156.3369\n",
      "Epoch[2/30], Step [100/148], Reconst Loss: 7326.4614, KL Div: 2220.1125\n",
      "Epoch[2/30], Step [110/148], Reconst Loss: 7164.0132, KL Div: 2289.6436\n",
      "Epoch[2/30], Step [120/148], Reconst Loss: 7246.4512, KL Div: 2214.3367\n",
      "Epoch[2/30], Step [130/148], Reconst Loss: 6853.1274, KL Div: 2186.3035\n",
      "Epoch[2/30], Step [140/148], Reconst Loss: 7085.8838, KL Div: 2339.4080\n",
      "train_loss : 7413.430993982263 test_loss : 32175.61474609375\n",
      "Epoch[3/30], Step [10/148], Reconst Loss: 6856.0269, KL Div: 2299.0588\n",
      "Epoch[3/30], Step [20/148], Reconst Loss: 6421.6021, KL Div: 2358.0674\n",
      "Epoch[3/30], Step [30/148], Reconst Loss: 7285.8032, KL Div: 2337.9883\n",
      "Epoch[3/30], Step [40/148], Reconst Loss: 6877.8428, KL Div: 2351.9089\n",
      "Epoch[3/30], Step [50/148], Reconst Loss: 6671.6221, KL Div: 2361.1948\n",
      "Epoch[3/30], Step [60/148], Reconst Loss: 6866.1631, KL Div: 2370.1870\n",
      "Epoch[3/30], Step [70/148], Reconst Loss: 6614.1841, KL Div: 2387.9924\n",
      "Epoch[3/30], Step [80/148], Reconst Loss: 6727.1699, KL Div: 2370.1746\n",
      "Epoch[3/30], Step [90/148], Reconst Loss: 6541.1523, KL Div: 2444.1697\n",
      "Epoch[3/30], Step [100/148], Reconst Loss: 6726.7925, KL Div: 2433.8076\n",
      "Epoch[3/30], Step [110/148], Reconst Loss: 6386.4849, KL Div: 2318.9258\n",
      "Epoch[3/30], Step [120/148], Reconst Loss: 6434.5542, KL Div: 2368.2747\n",
      "Epoch[3/30], Step [130/148], Reconst Loss: 6431.5801, KL Div: 2384.3345\n",
      "Epoch[3/30], Step [140/148], Reconst Loss: 6521.0381, KL Div: 2387.3186\n",
      "train_loss : 6661.788257495777 test_loss : 30339.037109375\n",
      "Epoch[4/30], Step [10/148], Reconst Loss: 6625.6055, KL Div: 2515.9976\n",
      "Epoch[4/30], Step [20/148], Reconst Loss: 6339.7246, KL Div: 2479.3452\n",
      "Epoch[4/30], Step [30/148], Reconst Loss: 6514.8809, KL Div: 2442.1089\n",
      "Epoch[4/30], Step [40/148], Reconst Loss: 6058.3984, KL Div: 2458.9998\n",
      "Epoch[4/30], Step [50/148], Reconst Loss: 6311.6313, KL Div: 2544.8669\n",
      "Epoch[4/30], Step [60/148], Reconst Loss: 6228.5024, KL Div: 2520.1616\n",
      "Epoch[4/30], Step [70/148], Reconst Loss: 6270.0312, KL Div: 2468.0469\n",
      "Epoch[4/30], Step [80/148], Reconst Loss: 6360.1392, KL Div: 2436.8560\n",
      "Epoch[4/30], Step [90/148], Reconst Loss: 6516.1006, KL Div: 2504.8347\n",
      "Epoch[4/30], Step [100/148], Reconst Loss: 6102.4307, KL Div: 2487.5962\n",
      "Epoch[4/30], Step [110/148], Reconst Loss: 6097.4878, KL Div: 2500.0718\n",
      "Epoch[4/30], Step [120/148], Reconst Loss: 6288.4438, KL Div: 2530.1992\n",
      "Epoch[4/30], Step [130/148], Reconst Loss: 6352.1421, KL Div: 2461.1880\n",
      "Epoch[4/30], Step [140/148], Reconst Loss: 6376.9092, KL Div: 2470.4531\n",
      "train_loss : 6323.466407569679 test_loss : 28595.2541015625\n",
      "Epoch[5/30], Step [10/148], Reconst Loss: 6054.4150, KL Div: 2550.7236\n",
      "Epoch[5/30], Step [20/148], Reconst Loss: 6199.8335, KL Div: 2495.7434\n",
      "Epoch[5/30], Step [30/148], Reconst Loss: 6203.5454, KL Div: 2495.0864\n",
      "Epoch[5/30], Step [40/148], Reconst Loss: 6449.7300, KL Div: 2403.7329\n",
      "Epoch[5/30], Step [50/148], Reconst Loss: 6041.3711, KL Div: 2507.6606\n",
      "Epoch[5/30], Step [60/148], Reconst Loss: 5948.7324, KL Div: 2592.0220\n",
      "Epoch[5/30], Step [70/148], Reconst Loss: 6264.0156, KL Div: 2650.7314\n",
      "Epoch[5/30], Step [80/148], Reconst Loss: 6035.0093, KL Div: 2485.7988\n",
      "Epoch[5/30], Step [90/148], Reconst Loss: 5849.4946, KL Div: 2550.8264\n",
      "Epoch[5/30], Step [100/148], Reconst Loss: 6283.8833, KL Div: 2515.5793\n",
      "Epoch[5/30], Step [110/148], Reconst Loss: 6120.2676, KL Div: 2639.9958\n",
      "Epoch[5/30], Step [120/148], Reconst Loss: 6050.8945, KL Div: 2630.3911\n",
      "Epoch[5/30], Step [130/148], Reconst Loss: 6204.5020, KL Div: 2550.2510\n",
      "Epoch[5/30], Step [140/148], Reconst Loss: 6080.3584, KL Div: 2556.7686\n",
      "train_loss : 6127.225968644426 test_loss : 27733.4140625\n",
      "Epoch[6/30], Step [10/148], Reconst Loss: 6160.0591, KL Div: 2645.4182\n",
      "Epoch[6/30], Step [20/148], Reconst Loss: 6165.2441, KL Div: 2627.7695\n",
      "Epoch[6/30], Step [30/148], Reconst Loss: 6098.3218, KL Div: 2600.5806\n",
      "Epoch[6/30], Step [40/148], Reconst Loss: 5754.5771, KL Div: 2547.3457\n",
      "Epoch[6/30], Step [50/148], Reconst Loss: 5941.4688, KL Div: 2572.0684\n",
      "Epoch[6/30], Step [60/148], Reconst Loss: 6110.3896, KL Div: 2583.5361\n",
      "Epoch[6/30], Step [70/148], Reconst Loss: 6015.1523, KL Div: 2497.6873\n",
      "Epoch[6/30], Step [80/148], Reconst Loss: 6212.2739, KL Div: 2578.5532\n",
      "Epoch[6/30], Step [90/148], Reconst Loss: 6218.6543, KL Div: 2520.4248\n",
      "Epoch[6/30], Step [100/148], Reconst Loss: 5950.9932, KL Div: 2551.8369\n",
      "Epoch[6/30], Step [110/148], Reconst Loss: 5912.8184, KL Div: 2585.1919\n",
      "Epoch[6/30], Step [120/148], Reconst Loss: 5914.3452, KL Div: 2649.8660\n",
      "Epoch[6/30], Step [130/148], Reconst Loss: 5959.4404, KL Div: 2553.3826\n",
      "Epoch[6/30], Step [140/148], Reconst Loss: 5941.1343, KL Div: 2603.7175\n",
      "train_loss : 5982.0485872835725 test_loss : 27570.365966796875\n",
      "Epoch[7/30], Step [10/148], Reconst Loss: 6224.9810, KL Div: 2508.1973\n",
      "Epoch[7/30], Step [20/148], Reconst Loss: 5896.6841, KL Div: 2624.1011\n",
      "Epoch[7/30], Step [30/148], Reconst Loss: 5830.1753, KL Div: 2633.5376\n",
      "Epoch[7/30], Step [40/148], Reconst Loss: 5917.2202, KL Div: 2622.9189\n",
      "Epoch[7/30], Step [50/148], Reconst Loss: 5929.7690, KL Div: 2531.6921\n",
      "Epoch[7/30], Step [60/148], Reconst Loss: 5938.4126, KL Div: 2542.6821\n",
      "Epoch[7/30], Step [70/148], Reconst Loss: 5854.0259, KL Div: 2576.6040\n",
      "Epoch[7/30], Step [80/148], Reconst Loss: 5958.2090, KL Div: 2608.5479\n",
      "Epoch[7/30], Step [90/148], Reconst Loss: 5936.2822, KL Div: 2669.3557\n",
      "Epoch[7/30], Step [100/148], Reconst Loss: 5812.0664, KL Div: 2514.6580\n",
      "Epoch[7/30], Step [110/148], Reconst Loss: 5766.2432, KL Div: 2623.6970\n",
      "Epoch[7/30], Step [120/148], Reconst Loss: 5747.8638, KL Div: 2549.2942\n",
      "Epoch[7/30], Step [130/148], Reconst Loss: 5843.4722, KL Div: 2589.2566\n",
      "Epoch[7/30], Step [140/148], Reconst Loss: 5930.6162, KL Div: 2654.8301\n",
      "train_loss : 5878.98736077386 test_loss : 26819.381884765626\n",
      "Epoch[8/30], Step [10/148], Reconst Loss: 5909.2544, KL Div: 2648.2307\n",
      "Epoch[8/30], Step [20/148], Reconst Loss: 5672.3877, KL Div: 2603.5684\n",
      "Epoch[8/30], Step [30/148], Reconst Loss: 5706.6475, KL Div: 2618.6282\n",
      "Epoch[8/30], Step [40/148], Reconst Loss: 5952.2222, KL Div: 2713.5247\n",
      "Epoch[8/30], Step [50/148], Reconst Loss: 5678.0542, KL Div: 2637.4624\n",
      "Epoch[8/30], Step [60/148], Reconst Loss: 5557.4561, KL Div: 2643.7666\n",
      "Epoch[8/30], Step [70/148], Reconst Loss: 5883.1270, KL Div: 2611.6685\n",
      "Epoch[8/30], Step [80/148], Reconst Loss: 5861.6230, KL Div: 2518.8215\n",
      "Epoch[8/30], Step [90/148], Reconst Loss: 5878.1133, KL Div: 2660.8726\n",
      "Epoch[8/30], Step [100/148], Reconst Loss: 5764.7524, KL Div: 2612.6655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[8/30], Step [110/148], Reconst Loss: 5827.9941, KL Div: 2663.0105\n",
      "Epoch[8/30], Step [120/148], Reconst Loss: 5730.9287, KL Div: 2679.3757\n",
      "Epoch[8/30], Step [130/148], Reconst Loss: 5863.6890, KL Div: 2673.4041\n",
      "Epoch[8/30], Step [140/148], Reconst Loss: 5771.9346, KL Div: 2620.0916\n",
      "train_loss : 5784.863545185811 test_loss : 26960.381396484376\n",
      "Epoch[9/30], Step [10/148], Reconst Loss: 5762.8467, KL Div: 2551.1548\n",
      "Epoch[9/30], Step [20/148], Reconst Loss: 5701.7847, KL Div: 2642.8545\n",
      "Epoch[9/30], Step [30/148], Reconst Loss: 5614.7363, KL Div: 2672.1494\n",
      "Epoch[9/30], Step [40/148], Reconst Loss: 5934.6978, KL Div: 2653.8779\n",
      "Epoch[9/30], Step [50/148], Reconst Loss: 5853.0151, KL Div: 2590.1536\n",
      "Epoch[9/30], Step [60/148], Reconst Loss: 5448.4595, KL Div: 2592.5586\n",
      "Epoch[9/30], Step [70/148], Reconst Loss: 5770.7617, KL Div: 2634.7842\n",
      "Epoch[9/30], Step [80/148], Reconst Loss: 5713.4033, KL Div: 2557.4189\n",
      "Epoch[9/30], Step [90/148], Reconst Loss: 5665.8657, KL Div: 2543.8350\n",
      "Epoch[9/30], Step [100/148], Reconst Loss: 5760.1689, KL Div: 2656.2317\n",
      "Epoch[9/30], Step [110/148], Reconst Loss: 5813.2070, KL Div: 2629.5549\n",
      "Epoch[9/30], Step [120/148], Reconst Loss: 5725.8618, KL Div: 2570.9663\n",
      "Epoch[9/30], Step [130/148], Reconst Loss: 5602.4678, KL Div: 2475.1792\n",
      "Epoch[9/30], Step [140/148], Reconst Loss: 5837.2363, KL Div: 2589.9614\n",
      "train_loss : 5716.260761982686 test_loss : 26702.066650390625\n",
      "Epoch[10/30], Step [10/148], Reconst Loss: 5711.8511, KL Div: 2649.3572\n",
      "Epoch[10/30], Step [20/148], Reconst Loss: 5437.6650, KL Div: 2610.5632\n",
      "Epoch[10/30], Step [30/148], Reconst Loss: 5837.9487, KL Div: 2644.9094\n",
      "Epoch[10/30], Step [40/148], Reconst Loss: 5593.8652, KL Div: 2658.5735\n",
      "Epoch[10/30], Step [50/148], Reconst Loss: 5427.3369, KL Div: 2567.5508\n",
      "Epoch[10/30], Step [60/148], Reconst Loss: 5659.1274, KL Div: 2794.6035\n",
      "Epoch[10/30], Step [70/148], Reconst Loss: 5477.5430, KL Div: 2580.6465\n",
      "Epoch[10/30], Step [80/148], Reconst Loss: 5482.4375, KL Div: 2673.3784\n",
      "Epoch[10/30], Step [90/148], Reconst Loss: 5557.7056, KL Div: 2620.1526\n",
      "Epoch[10/30], Step [100/148], Reconst Loss: 5560.9775, KL Div: 2678.1848\n",
      "Epoch[10/30], Step [110/148], Reconst Loss: 5857.8975, KL Div: 2654.6616\n",
      "Epoch[10/30], Step [120/148], Reconst Loss: 5417.4585, KL Div: 2529.2998\n",
      "Epoch[10/30], Step [130/148], Reconst Loss: 5612.4116, KL Div: 2550.0342\n",
      "Epoch[10/30], Step [140/148], Reconst Loss: 5966.6318, KL Div: 2624.3677\n",
      "train_loss : 5654.562948690878 test_loss : 26405.85859375\n",
      "Epoch[11/30], Step [10/148], Reconst Loss: 5639.8452, KL Div: 2571.7622\n",
      "Epoch[11/30], Step [20/148], Reconst Loss: 5747.2866, KL Div: 2710.1814\n",
      "Epoch[11/30], Step [30/148], Reconst Loss: 5705.7432, KL Div: 2661.2517\n",
      "Epoch[11/30], Step [40/148], Reconst Loss: 5514.7236, KL Div: 2720.8179\n",
      "Epoch[11/30], Step [50/148], Reconst Loss: 5466.1118, KL Div: 2638.2988\n",
      "Epoch[11/30], Step [60/148], Reconst Loss: 5745.1826, KL Div: 2531.6152\n",
      "Epoch[11/30], Step [70/148], Reconst Loss: 5790.2607, KL Div: 2523.4080\n",
      "Epoch[11/30], Step [80/148], Reconst Loss: 5756.6831, KL Div: 2553.5349\n",
      "Epoch[11/30], Step [90/148], Reconst Loss: 5638.8750, KL Div: 2688.3301\n",
      "Epoch[11/30], Step [100/148], Reconst Loss: 5522.8042, KL Div: 2676.2942\n",
      "Epoch[11/30], Step [110/148], Reconst Loss: 5640.5718, KL Div: 2628.1270\n",
      "Epoch[11/30], Step [120/148], Reconst Loss: 5445.3843, KL Div: 2691.8303\n",
      "Epoch[11/30], Step [130/148], Reconst Loss: 5573.7549, KL Div: 2650.7866\n",
      "Epoch[11/30], Step [140/148], Reconst Loss: 5494.8462, KL Div: 2535.2153\n",
      "train_loss : 5593.707611908784 test_loss : 25969.233837890624\n",
      "Epoch[12/30], Step [10/148], Reconst Loss: 5646.7158, KL Div: 2649.3542\n",
      "Epoch[12/30], Step [20/148], Reconst Loss: 5508.6846, KL Div: 2645.9246\n",
      "Epoch[12/30], Step [30/148], Reconst Loss: 5425.6445, KL Div: 2667.6206\n",
      "Epoch[12/30], Step [40/148], Reconst Loss: 5371.7720, KL Div: 2686.4888\n",
      "Epoch[12/30], Step [50/148], Reconst Loss: 5674.2534, KL Div: 2735.6924\n",
      "Epoch[12/30], Step [60/148], Reconst Loss: 5512.6509, KL Div: 2731.6760\n",
      "Epoch[12/30], Step [70/148], Reconst Loss: 5693.1919, KL Div: 2729.9160\n",
      "Epoch[12/30], Step [80/148], Reconst Loss: 5465.7578, KL Div: 2650.5757\n",
      "Epoch[12/30], Step [90/148], Reconst Loss: 5459.2925, KL Div: 2632.5652\n",
      "Epoch[12/30], Step [100/148], Reconst Loss: 5693.1567, KL Div: 2690.5889\n",
      "Epoch[12/30], Step [110/148], Reconst Loss: 5427.3169, KL Div: 2675.9475\n",
      "Epoch[12/30], Step [120/148], Reconst Loss: 5649.6270, KL Div: 2648.5979\n",
      "Epoch[12/30], Step [130/148], Reconst Loss: 5528.6865, KL Div: 2651.7898\n",
      "Epoch[12/30], Step [140/148], Reconst Loss: 5640.4097, KL Div: 2678.4048\n",
      "train_loss : 5547.732134844806 test_loss : 26079.631396484376\n",
      "Epoch[13/30], Step [10/148], Reconst Loss: 5366.4688, KL Div: 2727.0359\n",
      "Epoch[13/30], Step [20/148], Reconst Loss: 5284.9111, KL Div: 2742.5176\n",
      "Epoch[13/30], Step [30/148], Reconst Loss: 5645.2051, KL Div: 2686.0452\n",
      "Epoch[13/30], Step [40/148], Reconst Loss: 5472.6270, KL Div: 2649.5112\n",
      "Epoch[13/30], Step [50/148], Reconst Loss: 5408.5278, KL Div: 2626.2380\n",
      "Epoch[13/30], Step [60/148], Reconst Loss: 5385.3442, KL Div: 2665.9280\n",
      "Epoch[13/30], Step [70/148], Reconst Loss: 5425.1729, KL Div: 2726.0232\n",
      "Epoch[13/30], Step [80/148], Reconst Loss: 5504.1309, KL Div: 2557.4165\n",
      "Epoch[13/30], Step [90/148], Reconst Loss: 5607.8223, KL Div: 2664.9028\n",
      "Epoch[13/30], Step [100/148], Reconst Loss: 5391.5542, KL Div: 2679.7468\n",
      "Epoch[13/30], Step [110/148], Reconst Loss: 5540.1226, KL Div: 2628.5432\n",
      "Epoch[13/30], Step [120/148], Reconst Loss: 5724.7925, KL Div: 2686.2637\n",
      "Epoch[13/30], Step [130/148], Reconst Loss: 5477.0171, KL Div: 2663.4607\n",
      "Epoch[13/30], Step [140/148], Reconst Loss: 5420.1055, KL Div: 2678.9712\n",
      "train_loss : 5501.0250870988175 test_loss : 25393.599609375\n",
      "Epoch[14/30], Step [10/148], Reconst Loss: 5383.2310, KL Div: 2682.4189\n",
      "Epoch[14/30], Step [20/148], Reconst Loss: 5402.9282, KL Div: 2676.1140\n",
      "Epoch[14/30], Step [30/148], Reconst Loss: 5499.0737, KL Div: 2751.4729\n",
      "Epoch[14/30], Step [40/148], Reconst Loss: 5343.1787, KL Div: 2537.8271\n",
      "Epoch[14/30], Step [50/148], Reconst Loss: 5387.1436, KL Div: 2730.0840\n",
      "Epoch[14/30], Step [60/148], Reconst Loss: 5477.8071, KL Div: 2679.6318\n",
      "Epoch[14/30], Step [70/148], Reconst Loss: 5508.0679, KL Div: 2691.9856\n",
      "Epoch[14/30], Step [80/148], Reconst Loss: 5336.2007, KL Div: 2797.8181\n",
      "Epoch[14/30], Step [90/148], Reconst Loss: 5450.6343, KL Div: 2785.8857\n",
      "Epoch[14/30], Step [100/148], Reconst Loss: 5519.7974, KL Div: 2691.3174\n",
      "Epoch[14/30], Step [110/148], Reconst Loss: 5519.8711, KL Div: 2598.2786\n",
      "Epoch[14/30], Step [120/148], Reconst Loss: 5638.4351, KL Div: 2606.6997\n",
      "Epoch[14/30], Step [130/148], Reconst Loss: 5506.8882, KL Div: 2743.4988\n",
      "Epoch[14/30], Step [140/148], Reconst Loss: 5728.9873, KL Div: 2731.2634\n",
      "train_loss : 5467.525304845862 test_loss : 25423.957568359376\n",
      "Epoch[15/30], Step [10/148], Reconst Loss: 5283.5220, KL Div: 2605.9934\n",
      "Epoch[15/30], Step [20/148], Reconst Loss: 5342.5562, KL Div: 2614.0435\n",
      "Epoch[15/30], Step [30/148], Reconst Loss: 5630.3457, KL Div: 2686.4624\n",
      "Epoch[15/30], Step [40/148], Reconst Loss: 5507.6172, KL Div: 2772.7922\n",
      "Epoch[15/30], Step [50/148], Reconst Loss: 5480.5117, KL Div: 2729.4722\n",
      "Epoch[15/30], Step [60/148], Reconst Loss: 5279.0396, KL Div: 2623.6919\n",
      "Epoch[15/30], Step [70/148], Reconst Loss: 5425.8608, KL Div: 2549.6248\n",
      "Epoch[15/30], Step [80/148], Reconst Loss: 5676.2168, KL Div: 2523.3813\n",
      "Epoch[15/30], Step [90/148], Reconst Loss: 5227.2202, KL Div: 2681.6934\n",
      "Epoch[15/30], Step [100/148], Reconst Loss: 5633.9155, KL Div: 2717.4092\n",
      "Epoch[15/30], Step [110/148], Reconst Loss: 5549.0410, KL Div: 2795.0791\n",
      "Epoch[15/30], Step [120/148], Reconst Loss: 5395.1919, KL Div: 2727.1069\n",
      "Epoch[15/30], Step [130/148], Reconst Loss: 5217.2710, KL Div: 2766.8259\n",
      "Epoch[15/30], Step [140/148], Reconst Loss: 5335.8369, KL Div: 2626.6230\n",
      "train_loss : 5429.371971336571 test_loss : 25975.48974609375\n",
      "Epoch[16/30], Step [10/148], Reconst Loss: 5373.0620, KL Div: 2702.5134\n",
      "Epoch[16/30], Step [20/148], Reconst Loss: 5283.4419, KL Div: 2733.7695\n",
      "Epoch[16/30], Step [30/148], Reconst Loss: 5224.1899, KL Div: 2739.3950\n",
      "Epoch[16/30], Step [40/148], Reconst Loss: 5546.4541, KL Div: 2791.3262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[16/30], Step [50/148], Reconst Loss: 5499.3770, KL Div: 2659.0552\n",
      "Epoch[16/30], Step [60/148], Reconst Loss: 5421.1279, KL Div: 2644.7993\n",
      "Epoch[16/30], Step [70/148], Reconst Loss: 5299.8750, KL Div: 2607.6143\n",
      "Epoch[16/30], Step [80/148], Reconst Loss: 5486.2568, KL Div: 2608.1404\n",
      "Epoch[16/30], Step [90/148], Reconst Loss: 5503.6270, KL Div: 2671.8789\n",
      "Epoch[16/30], Step [100/148], Reconst Loss: 5433.2139, KL Div: 2724.9434\n",
      "Epoch[16/30], Step [110/148], Reconst Loss: 5604.0737, KL Div: 2640.0745\n",
      "Epoch[16/30], Step [120/148], Reconst Loss: 5343.8418, KL Div: 2701.4805\n",
      "Epoch[16/30], Step [130/148], Reconst Loss: 5609.1689, KL Div: 2821.9563\n",
      "Epoch[16/30], Step [140/148], Reconst Loss: 5530.8154, KL Div: 2613.4109\n",
      "train_loss : 5400.796277845228 test_loss : 25374.53408203125\n",
      "Epoch[17/30], Step [10/148], Reconst Loss: 5472.2476, KL Div: 2713.6677\n",
      "Epoch[17/30], Step [20/148], Reconst Loss: 5465.7456, KL Div: 2696.8298\n",
      "Epoch[17/30], Step [30/148], Reconst Loss: 5471.6201, KL Div: 2730.4858\n",
      "Epoch[17/30], Step [40/148], Reconst Loss: 5510.8560, KL Div: 2744.4656\n",
      "Epoch[17/30], Step [50/148], Reconst Loss: 5423.1685, KL Div: 2747.0474\n",
      "Epoch[17/30], Step [60/148], Reconst Loss: 5371.3857, KL Div: 2739.6985\n",
      "Epoch[17/30], Step [70/148], Reconst Loss: 5314.3389, KL Div: 2725.0852\n",
      "Epoch[17/30], Step [80/148], Reconst Loss: 5460.3535, KL Div: 2639.0452\n",
      "Epoch[17/30], Step [90/148], Reconst Loss: 5202.5386, KL Div: 2599.6948\n",
      "Epoch[17/30], Step [100/148], Reconst Loss: 5393.5791, KL Div: 2625.8887\n",
      "Epoch[17/30], Step [110/148], Reconst Loss: 5447.2573, KL Div: 2695.8320\n",
      "Epoch[17/30], Step [120/148], Reconst Loss: 5629.3213, KL Div: 2657.3889\n",
      "Epoch[17/30], Step [130/148], Reconst Loss: 5217.4370, KL Div: 2776.6138\n",
      "Epoch[17/30], Step [140/148], Reconst Loss: 5318.6040, KL Div: 2689.7031\n",
      "train_loss : 5368.8498007284625 test_loss : 25116.9439453125\n",
      "Epoch[18/30], Step [10/148], Reconst Loss: 5282.5894, KL Div: 2619.2205\n",
      "Epoch[18/30], Step [20/148], Reconst Loss: 5348.3428, KL Div: 2727.5298\n",
      "Epoch[18/30], Step [30/148], Reconst Loss: 5291.4004, KL Div: 2758.8728\n",
      "Epoch[18/30], Step [40/148], Reconst Loss: 5373.4517, KL Div: 2641.3440\n",
      "Epoch[18/30], Step [50/148], Reconst Loss: 5399.7646, KL Div: 2685.3557\n",
      "Epoch[18/30], Step [60/148], Reconst Loss: 5443.7153, KL Div: 2667.9607\n",
      "Epoch[18/30], Step [70/148], Reconst Loss: 5410.2632, KL Div: 2763.9324\n",
      "Epoch[18/30], Step [80/148], Reconst Loss: 5495.1543, KL Div: 2743.5059\n",
      "Epoch[18/30], Step [90/148], Reconst Loss: 5481.4316, KL Div: 2774.2698\n",
      "Epoch[18/30], Step [100/148], Reconst Loss: 5518.7798, KL Div: 2698.1331\n",
      "Epoch[18/30], Step [110/148], Reconst Loss: 5373.2749, KL Div: 2696.5325\n",
      "Epoch[18/30], Step [120/148], Reconst Loss: 5208.2168, KL Div: 2681.3936\n",
      "Epoch[18/30], Step [130/148], Reconst Loss: 5362.9761, KL Div: 2684.5544\n",
      "Epoch[18/30], Step [140/148], Reconst Loss: 5223.3555, KL Div: 2689.3286\n",
      "train_loss : 5340.411205394848 test_loss : 25592.348681640626\n",
      "Epoch[19/30], Step [10/148], Reconst Loss: 5157.4609, KL Div: 2699.4058\n",
      "Epoch[19/30], Step [20/148], Reconst Loss: 5268.0894, KL Div: 2761.6294\n",
      "Epoch[19/30], Step [30/148], Reconst Loss: 5312.5063, KL Div: 2649.3689\n",
      "Epoch[19/30], Step [40/148], Reconst Loss: 5277.0903, KL Div: 2691.2705\n",
      "Epoch[19/30], Step [50/148], Reconst Loss: 5209.4614, KL Div: 2748.7053\n",
      "Epoch[19/30], Step [60/148], Reconst Loss: 5122.7622, KL Div: 2685.0405\n",
      "Epoch[19/30], Step [70/148], Reconst Loss: 5227.9106, KL Div: 2742.4673\n",
      "Epoch[19/30], Step [80/148], Reconst Loss: 5519.8896, KL Div: 2705.6511\n",
      "Epoch[19/30], Step [90/148], Reconst Loss: 5331.5356, KL Div: 2711.4297\n",
      "Epoch[19/30], Step [100/148], Reconst Loss: 5176.0547, KL Div: 2739.7971\n",
      "Epoch[19/30], Step [110/148], Reconst Loss: 5271.6104, KL Div: 2652.9348\n",
      "Epoch[19/30], Step [120/148], Reconst Loss: 5433.8354, KL Div: 2624.7964\n",
      "Epoch[19/30], Step [130/148], Reconst Loss: 5324.5220, KL Div: 2768.5513\n",
      "Epoch[19/30], Step [140/148], Reconst Loss: 5450.2031, KL Div: 2727.9849\n",
      "train_loss : 5320.052467139992 test_loss : 25273.5916015625\n",
      "Epoch[20/30], Step [10/148], Reconst Loss: 5303.0679, KL Div: 2750.1157\n",
      "Epoch[20/30], Step [20/148], Reconst Loss: 4964.5825, KL Div: 2676.0093\n",
      "Epoch[20/30], Step [30/148], Reconst Loss: 5338.8784, KL Div: 2829.3616\n",
      "Epoch[20/30], Step [40/148], Reconst Loss: 5396.1060, KL Div: 2675.1572\n",
      "Epoch[20/30], Step [50/148], Reconst Loss: 5368.7720, KL Div: 2722.8340\n",
      "Epoch[20/30], Step [60/148], Reconst Loss: 5299.7715, KL Div: 2722.4019\n",
      "Epoch[20/30], Step [70/148], Reconst Loss: 5155.0913, KL Div: 2703.2141\n",
      "Epoch[20/30], Step [80/148], Reconst Loss: 5140.2114, KL Div: 2782.5454\n",
      "Epoch[20/30], Step [90/148], Reconst Loss: 5407.5601, KL Div: 2736.6582\n",
      "Epoch[20/30], Step [100/148], Reconst Loss: 5500.4771, KL Div: 2643.0447\n",
      "Epoch[20/30], Step [110/148], Reconst Loss: 5549.6890, KL Div: 2707.7966\n",
      "Epoch[20/30], Step [120/148], Reconst Loss: 5244.0244, KL Div: 2719.5254\n",
      "Epoch[20/30], Step [130/148], Reconst Loss: 5122.7622, KL Div: 2699.9060\n",
      "Epoch[20/30], Step [140/148], Reconst Loss: 5168.3472, KL Div: 2596.4414\n",
      "train_loss : 5293.645461623733 test_loss : 25147.659326171874\n",
      "Epoch[21/30], Step [10/148], Reconst Loss: 5161.7783, KL Div: 2704.8325\n",
      "Epoch[21/30], Step [20/148], Reconst Loss: 5401.0435, KL Div: 2607.5662\n",
      "Epoch[21/30], Step [30/148], Reconst Loss: 5141.7256, KL Div: 2666.5615\n",
      "Epoch[21/30], Step [40/148], Reconst Loss: 5373.8442, KL Div: 2740.6162\n",
      "Epoch[21/30], Step [50/148], Reconst Loss: 5419.5479, KL Div: 2805.6221\n",
      "Epoch[21/30], Step [60/148], Reconst Loss: 5393.8301, KL Div: 2674.4219\n",
      "Epoch[21/30], Step [70/148], Reconst Loss: 5093.9399, KL Div: 2778.6270\n",
      "Epoch[21/30], Step [80/148], Reconst Loss: 5266.4438, KL Div: 2737.8174\n",
      "Epoch[21/30], Step [90/148], Reconst Loss: 5174.1685, KL Div: 2598.0356\n",
      "Epoch[21/30], Step [100/148], Reconst Loss: 5260.9541, KL Div: 2608.1956\n",
      "Epoch[21/30], Step [110/148], Reconst Loss: 5140.0562, KL Div: 2714.4377\n",
      "Epoch[21/30], Step [120/148], Reconst Loss: 5275.4170, KL Div: 2747.3457\n",
      "Epoch[21/30], Step [130/148], Reconst Loss: 5257.4502, KL Div: 2780.4956\n",
      "Epoch[21/30], Step [140/148], Reconst Loss: 5247.7397, KL Div: 2605.4082\n",
      "train_loss : 5272.2124716269 test_loss : 25315.356103515624\n",
      "Epoch[22/30], Step [10/148], Reconst Loss: 5102.6982, KL Div: 2723.3062\n",
      "Epoch[22/30], Step [20/148], Reconst Loss: 5251.0356, KL Div: 2644.5544\n",
      "Epoch[22/30], Step [30/148], Reconst Loss: 5107.7607, KL Div: 2586.4805\n",
      "Epoch[22/30], Step [40/148], Reconst Loss: 5558.7837, KL Div: 2684.6008\n",
      "Epoch[22/30], Step [50/148], Reconst Loss: 5163.7739, KL Div: 2724.5166\n",
      "Epoch[22/30], Step [60/148], Reconst Loss: 5135.9976, KL Div: 2733.5898\n",
      "Epoch[22/30], Step [70/148], Reconst Loss: 5284.4854, KL Div: 2701.8689\n",
      "Epoch[22/30], Step [80/148], Reconst Loss: 5368.0938, KL Div: 2665.7966\n",
      "Epoch[22/30], Step [90/148], Reconst Loss: 5172.9170, KL Div: 2672.2004\n",
      "Epoch[22/30], Step [100/148], Reconst Loss: 5075.7021, KL Div: 2666.7280\n",
      "Epoch[22/30], Step [110/148], Reconst Loss: 5513.7383, KL Div: 2773.3589\n",
      "Epoch[22/30], Step [120/148], Reconst Loss: 5251.2354, KL Div: 2813.1067\n",
      "Epoch[22/30], Step [130/148], Reconst Loss: 5254.6597, KL Div: 2697.6130\n",
      "Epoch[22/30], Step [140/148], Reconst Loss: 5283.6396, KL Div: 2758.4124\n",
      "train_loss : 5253.8010352882175 test_loss : 25321.1232421875\n",
      "Epoch[23/30], Step [10/148], Reconst Loss: 5169.3872, KL Div: 2715.1606\n",
      "Epoch[23/30], Step [20/148], Reconst Loss: 5180.4736, KL Div: 2715.2026\n",
      "Epoch[23/30], Step [30/148], Reconst Loss: 4964.5142, KL Div: 2788.3167\n",
      "Epoch[23/30], Step [40/148], Reconst Loss: 5197.6216, KL Div: 2603.8308\n",
      "Epoch[23/30], Step [50/148], Reconst Loss: 5345.1948, KL Div: 2749.4395\n",
      "Epoch[23/30], Step [60/148], Reconst Loss: 5106.9844, KL Div: 2681.9438\n",
      "Epoch[23/30], Step [70/148], Reconst Loss: 5231.6797, KL Div: 2791.0264\n",
      "Epoch[23/30], Step [80/148], Reconst Loss: 5078.6606, KL Div: 2674.2002\n",
      "Epoch[23/30], Step [90/148], Reconst Loss: 5157.6978, KL Div: 2766.8721\n",
      "Epoch[23/30], Step [100/148], Reconst Loss: 5238.1934, KL Div: 2755.9324\n",
      "Epoch[23/30], Step [110/148], Reconst Loss: 5247.2163, KL Div: 2722.9062\n",
      "Epoch[23/30], Step [120/148], Reconst Loss: 4955.8828, KL Div: 2682.2341\n",
      "Epoch[23/30], Step [130/148], Reconst Loss: 5122.9180, KL Div: 2763.1096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/30], Step [140/148], Reconst Loss: 5453.4390, KL Div: 2713.7427\n",
      "train_loss : 5234.064730257602 test_loss : 25065.456005859374\n",
      "Epoch[24/30], Step [10/148], Reconst Loss: 5246.2612, KL Div: 2687.4470\n",
      "Epoch[24/30], Step [20/148], Reconst Loss: 4929.4072, KL Div: 2688.0552\n",
      "Epoch[24/30], Step [30/148], Reconst Loss: 5056.8643, KL Div: 2647.7634\n",
      "Epoch[24/30], Step [40/148], Reconst Loss: 5202.7300, KL Div: 2687.7334\n",
      "Epoch[24/30], Step [50/148], Reconst Loss: 5177.0615, KL Div: 2736.9475\n",
      "Epoch[24/30], Step [60/148], Reconst Loss: 5185.5991, KL Div: 2734.7986\n",
      "Epoch[24/30], Step [70/148], Reconst Loss: 5073.8726, KL Div: 2755.2161\n",
      "Epoch[24/30], Step [80/148], Reconst Loss: 5270.1533, KL Div: 2779.8892\n",
      "Epoch[24/30], Step [90/148], Reconst Loss: 5207.9502, KL Div: 2712.8711\n",
      "Epoch[24/30], Step [100/148], Reconst Loss: 5092.7993, KL Div: 2729.0596\n",
      "Epoch[24/30], Step [110/148], Reconst Loss: 5294.8838, KL Div: 2662.7344\n",
      "Epoch[24/30], Step [120/148], Reconst Loss: 5495.9492, KL Div: 2641.7085\n",
      "Epoch[24/30], Step [130/148], Reconst Loss: 5387.8433, KL Div: 2670.3049\n",
      "Epoch[24/30], Step [140/148], Reconst Loss: 5458.8052, KL Div: 2652.3164\n",
      "train_loss : 5219.541358741554 test_loss : 24848.991162109374\n",
      "Epoch[25/30], Step [10/148], Reconst Loss: 5091.7271, KL Div: 2751.6045\n",
      "Epoch[25/30], Step [20/148], Reconst Loss: 5241.7358, KL Div: 2748.2883\n",
      "Epoch[25/30], Step [30/148], Reconst Loss: 5374.9878, KL Div: 2715.6094\n",
      "Epoch[25/30], Step [40/148], Reconst Loss: 5064.1870, KL Div: 2656.6809\n",
      "Epoch[25/30], Step [50/148], Reconst Loss: 5407.4429, KL Div: 2767.3740\n",
      "Epoch[25/30], Step [60/148], Reconst Loss: 5152.0933, KL Div: 2658.4993\n",
      "Epoch[25/30], Step [70/148], Reconst Loss: 5233.8374, KL Div: 2614.0967\n",
      "Epoch[25/30], Step [80/148], Reconst Loss: 5359.1348, KL Div: 2716.4375\n",
      "Epoch[25/30], Step [90/148], Reconst Loss: 5194.4565, KL Div: 2823.0117\n",
      "Epoch[25/30], Step [100/148], Reconst Loss: 5002.0527, KL Div: 2701.9563\n",
      "Epoch[25/30], Step [110/148], Reconst Loss: 5282.9302, KL Div: 2650.6792\n",
      "Epoch[25/30], Step [120/148], Reconst Loss: 5383.1221, KL Div: 2803.8406\n",
      "Epoch[25/30], Step [130/148], Reconst Loss: 5289.7417, KL Div: 2712.4263\n",
      "Epoch[25/30], Step [140/148], Reconst Loss: 5144.8823, KL Div: 2702.4185\n",
      "train_loss : 5205.9888850031675 test_loss : 24982.942529296874\n",
      "Epoch[26/30], Step [10/148], Reconst Loss: 5266.5269, KL Div: 2669.8276\n",
      "Epoch[26/30], Step [20/148], Reconst Loss: 5039.9111, KL Div: 2646.9592\n",
      "Epoch[26/30], Step [30/148], Reconst Loss: 5032.8257, KL Div: 2831.9324\n",
      "Epoch[26/30], Step [40/148], Reconst Loss: 4975.2612, KL Div: 2715.9937\n",
      "Epoch[26/30], Step [50/148], Reconst Loss: 5195.0703, KL Div: 2690.9055\n",
      "Epoch[26/30], Step [60/148], Reconst Loss: 5164.8032, KL Div: 2734.6743\n",
      "Epoch[26/30], Step [70/148], Reconst Loss: 5240.5342, KL Div: 2715.2375\n",
      "Epoch[26/30], Step [80/148], Reconst Loss: 5366.3560, KL Div: 2674.5020\n",
      "Epoch[26/30], Step [90/148], Reconst Loss: 5366.6128, KL Div: 2703.1987\n",
      "Epoch[26/30], Step [100/148], Reconst Loss: 4878.2876, KL Div: 2850.7964\n",
      "Epoch[26/30], Step [110/148], Reconst Loss: 5312.3271, KL Div: 2764.0464\n",
      "Epoch[26/30], Step [120/148], Reconst Loss: 5246.4062, KL Div: 2743.4412\n",
      "Epoch[26/30], Step [130/148], Reconst Loss: 5327.0757, KL Div: 2664.4800\n",
      "Epoch[26/30], Step [140/148], Reconst Loss: 5282.4463, KL Div: 2725.8433\n",
      "train_loss : 5186.135342984586 test_loss : 24696.35947265625\n",
      "Epoch[27/30], Step [10/148], Reconst Loss: 5207.7954, KL Div: 2718.5437\n",
      "Epoch[27/30], Step [20/148], Reconst Loss: 5175.0581, KL Div: 2714.1562\n",
      "Epoch[27/30], Step [30/148], Reconst Loss: 5253.3330, KL Div: 2697.0444\n",
      "Epoch[27/30], Step [40/148], Reconst Loss: 5132.6719, KL Div: 2736.0417\n",
      "Epoch[27/30], Step [50/148], Reconst Loss: 5294.3613, KL Div: 2693.6208\n",
      "Epoch[27/30], Step [60/148], Reconst Loss: 5286.9341, KL Div: 2716.3901\n",
      "Epoch[27/30], Step [70/148], Reconst Loss: 5219.2539, KL Div: 2686.4517\n",
      "Epoch[27/30], Step [80/148], Reconst Loss: 5226.2515, KL Div: 2789.5178\n",
      "Epoch[27/30], Step [90/148], Reconst Loss: 5144.8164, KL Div: 2685.1812\n",
      "Epoch[27/30], Step [100/148], Reconst Loss: 5291.3535, KL Div: 2715.2932\n",
      "Epoch[27/30], Step [110/148], Reconst Loss: 5140.5957, KL Div: 2732.9214\n",
      "Epoch[27/30], Step [120/148], Reconst Loss: 5207.3975, KL Div: 2691.7629\n",
      "Epoch[27/30], Step [130/148], Reconst Loss: 5079.0137, KL Div: 2775.5015\n",
      "Epoch[27/30], Step [140/148], Reconst Loss: 5177.9639, KL Div: 2774.5054\n",
      "train_loss : 5168.218789590372 test_loss : 24737.328759765624\n",
      "Epoch[28/30], Step [10/148], Reconst Loss: 5124.1655, KL Div: 2703.2810\n",
      "Epoch[28/30], Step [20/148], Reconst Loss: 5198.2041, KL Div: 2711.9597\n",
      "Epoch[28/30], Step [30/148], Reconst Loss: 5155.5776, KL Div: 2664.7151\n",
      "Epoch[28/30], Step [40/148], Reconst Loss: 5400.7402, KL Div: 2738.8389\n",
      "Epoch[28/30], Step [50/148], Reconst Loss: 5147.2148, KL Div: 2757.0881\n",
      "Epoch[28/30], Step [60/148], Reconst Loss: 5256.7246, KL Div: 2740.8433\n",
      "Epoch[28/30], Step [70/148], Reconst Loss: 5165.2754, KL Div: 2611.6606\n",
      "Epoch[28/30], Step [80/148], Reconst Loss: 5105.5547, KL Div: 2648.2368\n",
      "Epoch[28/30], Step [90/148], Reconst Loss: 5290.1606, KL Div: 2696.9038\n",
      "Epoch[28/30], Step [100/148], Reconst Loss: 5467.8120, KL Div: 2798.8711\n",
      "Epoch[28/30], Step [110/148], Reconst Loss: 5164.9033, KL Div: 2712.5525\n",
      "Epoch[28/30], Step [120/148], Reconst Loss: 5030.9775, KL Div: 2743.1216\n",
      "Epoch[28/30], Step [130/148], Reconst Loss: 5278.2036, KL Div: 2690.1035\n",
      "Epoch[28/30], Step [140/148], Reconst Loss: 5420.0864, KL Div: 2719.7380\n",
      "train_loss : 5158.9055835620775 test_loss : 24498.39970703125\n",
      "Epoch[29/30], Step [10/148], Reconst Loss: 5152.8403, KL Div: 2637.6753\n",
      "Epoch[29/30], Step [20/148], Reconst Loss: 5251.0435, KL Div: 2722.6284\n",
      "Epoch[29/30], Step [30/148], Reconst Loss: 4991.3921, KL Div: 2761.6555\n",
      "Epoch[29/30], Step [40/148], Reconst Loss: 5220.5425, KL Div: 2836.3352\n",
      "Epoch[29/30], Step [50/148], Reconst Loss: 5119.2607, KL Div: 2780.5632\n",
      "Epoch[29/30], Step [60/148], Reconst Loss: 5281.4697, KL Div: 2718.1296\n",
      "Epoch[29/30], Step [70/148], Reconst Loss: 5075.3823, KL Div: 2619.2939\n",
      "Epoch[29/30], Step [80/148], Reconst Loss: 5211.3945, KL Div: 2778.3550\n",
      "Epoch[29/30], Step [90/148], Reconst Loss: 5281.1846, KL Div: 2745.9019\n",
      "Epoch[29/30], Step [100/148], Reconst Loss: 5200.8428, KL Div: 2889.8359\n",
      "Epoch[29/30], Step [110/148], Reconst Loss: 5321.9912, KL Div: 2848.8135\n",
      "Epoch[29/30], Step [120/148], Reconst Loss: 5253.0864, KL Div: 2668.4067\n",
      "Epoch[29/30], Step [130/148], Reconst Loss: 5054.3291, KL Div: 2601.7336\n",
      "Epoch[29/30], Step [140/148], Reconst Loss: 4957.2700, KL Div: 2835.6719\n",
      "train_loss : 5145.19080579603 test_loss : 24809.039599609376\n",
      "Epoch[30/30], Step [10/148], Reconst Loss: 4864.4331, KL Div: 2664.7368\n",
      "Epoch[30/30], Step [20/148], Reconst Loss: 5213.4907, KL Div: 2826.4209\n",
      "Epoch[30/30], Step [30/148], Reconst Loss: 5092.8682, KL Div: 2693.5710\n",
      "Epoch[30/30], Step [40/148], Reconst Loss: 5176.2148, KL Div: 2739.0471\n",
      "Epoch[30/30], Step [50/148], Reconst Loss: 5090.8511, KL Div: 2751.1648\n",
      "Epoch[30/30], Step [60/148], Reconst Loss: 4949.3667, KL Div: 2706.9219\n",
      "Epoch[30/30], Step [70/148], Reconst Loss: 5111.6167, KL Div: 2737.4514\n",
      "Epoch[30/30], Step [80/148], Reconst Loss: 5062.7427, KL Div: 2708.2480\n",
      "Epoch[30/30], Step [90/148], Reconst Loss: 5123.8979, KL Div: 2786.3752\n",
      "Epoch[30/30], Step [100/148], Reconst Loss: 4971.4473, KL Div: 2787.5847\n",
      "Epoch[30/30], Step [110/148], Reconst Loss: 5080.2617, KL Div: 2714.7104\n",
      "Epoch[30/30], Step [120/148], Reconst Loss: 4953.0791, KL Div: 2710.8096\n",
      "Epoch[30/30], Step [130/148], Reconst Loss: 5082.0571, KL Div: 2766.1025\n",
      "Epoch[30/30], Step [140/148], Reconst Loss: 5208.6724, KL Div: 2753.5393\n",
      "train_loss : 5130.109698321368 test_loss : 24501.011181640624\n"
     ]
    }
   ],
   "source": [
    "best_reconst_loss = 99999999999999999\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "test_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    tmp= 0\n",
    "    cnt = 0\n",
    "    for i, x in enumerate(trainset):\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        reconst_loss = F.mse_loss(x_reconst, x, size_average=False)\n",
    "        tmp  += reconst_loss.item()\n",
    "        cnt += 1\n",
    "        kl_div = -0.5 * torch.sum(1+log_var- mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        loss = reconst_loss+kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\"\n",
    "                 .format(epoch+1, num_epochs, i+1, len(trainset), reconst_loss.item(), kl_div.item()))\n",
    "    train_loss += [tmp/cnt]\n",
    "    \n",
    "    valid_reconst_loss = 0\n",
    "    cnt = 0\n",
    "    for j, y in enumerate(validset):\n",
    "        y_reconst, _, _ = model(y)\n",
    "        valid_reconst_loss += F.mse_loss(y_reconst, y, size_average=False)\n",
    "        cnt += 1\n",
    "    valid_loss = [valid_loss, valid_reconst_loss.item()/cnt]\n",
    "    if (valid_reconst_loss < best_reconst_loss):\n",
    "        best_model = model\n",
    "        best_reconst_loss = valid_reconst_loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        test_reconst_loss = 0\n",
    "        cnt = 0\n",
    "        for i, x in enumerate(testset):\n",
    "            label = x[:,784]\n",
    "            x = x[:,:784]\n",
    "            x_reconst, mu, log_var = best_model(x)\n",
    "            \n",
    "            reconst_loss = F.mse_loss(x_reconst, x, size_average=False)\n",
    "            cnt += 1\n",
    "            test_reconst_loss += reconst_loss.item()\n",
    "        test_loss += [test_reconst_loss/cnt]\n",
    "    print(\"train_loss :\", train_loss[epoch], \"test_loss :\", test_loss[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.legend(['train loss',' test loss'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
