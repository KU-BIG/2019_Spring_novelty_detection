{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from vae import VAE\n",
    "from loss_function import loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "input_dim = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# file located at '/data' in parent directory\n",
    "path = os.path.abspath(os.path.join(os.getcwd(), '..', 'data', 'creditcard.csv')) \n",
    "df = pd.read_csv(path)\n",
    "df['Time_diff_from_previous'] = df['Time'].diff()\n",
    "df['Time_diff_from_previous'].iloc[0] = 0\n",
    "df['Time_diff_from_last'] = df['Time'].diff().iloc[1:].reset_index(drop=True)\n",
    "df['Time_diff_from_last'].iloc[-1] = 0\n",
    "\n",
    "for column in df.columns[0:30] : \n",
    "    df[column] = df[column] / (df[column].max() - df[column].min())\n",
    "df['Time_diff_from_previous'] = df['Time_diff_from_previous'] / (df['Time_diff_from_previous'].max() - df['Time_diff_from_previous'].min())\n",
    "df['Time_diff_from_last'] = df['Time_diff_from_last'] / (df['Time_diff_from_last'].max() - df['Time_diff_from_last'].min())\n",
    "\n",
    "df = df.drop(['Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>Time_diff_from_previous</th>\n",
       "      <th>Time_diff_from_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023101</td>\n",
       "      <td>-0.000768</td>\n",
       "      <td>0.043951</td>\n",
       "      <td>0.061092</td>\n",
       "      <td>-0.002278</td>\n",
       "      <td>0.004649</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.012532</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>0.009019</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>-0.030892</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>-0.000427</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020248</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.019866</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>-0.000828</td>\n",
       "      <td>-0.000480</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>-0.008799</td>\n",
       "      <td>-0.003455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-0.045794</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.020565</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.023077</td>\n",
       "      <td>-0.014141</td>\n",
       "      <td>0.030727</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>-0.003388</td>\n",
       "      <td>0.018102</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>-0.052177</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013506</td>\n",
       "      <td>-0.092880</td>\n",
       "      <td>-0.018391</td>\n",
       "      <td>-0.022721</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.001213</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.016416</td>\n",
       "      <td>-0.001954</td>\n",
       "      <td>0.031070</td>\n",
       "      <td>-0.038269</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>-0.047781</td>\n",
       "      <td>-0.001137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002826</td>\n",
       "      <td>-0.158408</td>\n",
       "      <td>0.036339</td>\n",
       "      <td>-0.036252</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.03125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.019677</td>\n",
       "      <td>0.009261</td>\n",
       "      <td>0.026837</td>\n",
       "      <td>0.017866</td>\n",
       "      <td>-0.002741</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.003612</td>\n",
       "      <td>-0.002902</td>\n",
       "      <td>0.028170</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002041</td>\n",
       "      <td>0.019036</td>\n",
       "      <td>-0.011564</td>\n",
       "      <td>0.082048</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.023101 -0.000768  0.043951  0.061092 -0.002278  0.004649  0.001460   \n",
       "1  0.020248  0.002808  0.002885  0.019866  0.000404 -0.000828 -0.000480   \n",
       "2 -0.023077 -0.014141  0.030727  0.016835 -0.003388  0.018102  0.004822   \n",
       "3 -0.016416 -0.001954  0.031070 -0.038269 -0.000069  0.012539  0.001448   \n",
       "4 -0.019677  0.009261  0.026837  0.017866 -0.002741  0.000964  0.003612   \n",
       "\n",
       "         V8        V9       V10  ...       V23       V24       V25       V26  \\\n",
       "0  0.001059  0.012532  0.001878  ... -0.001641  0.009019  0.007215 -0.030892   \n",
       "1  0.000913 -0.008799 -0.003455  ...  0.001504 -0.045794  0.009384  0.020565   \n",
       "2  0.002657 -0.052177  0.004296  ...  0.013506 -0.092880 -0.018391 -0.022721   \n",
       "3  0.004049 -0.047781 -0.001137  ... -0.002826 -0.158408  0.036339 -0.036252   \n",
       "4 -0.002902  0.028170  0.015581  ... -0.002041  0.019036 -0.011564  0.082048   \n",
       "\n",
       "        V27       V28    Amount  Class  Time_diff_from_previous  \\\n",
       "0  0.002465 -0.000427  0.005824      0                  0.00000   \n",
       "1 -0.000166  0.000299  0.000105      0                  0.00000   \n",
       "2 -0.001022 -0.001213  0.014739      0                  0.03125   \n",
       "3  0.001158  0.001247  0.004807      0                  0.00000   \n",
       "4  0.004050  0.004366  0.002724      0                  0.03125   \n",
       "\n",
       "   Time_diff_from_last  \n",
       "0              0.00000  \n",
       "1              0.03125  \n",
       "2              0.00000  \n",
       "3              0.03125  \n",
       "4              0.00000  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = [0.6, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCFDataset(Dataset) : \n",
    "    def __init__(self, df, transform=transforms.ToTensor()) :\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        # x, y\n",
    "        x = torch.from_numpy(self.df.drop(['Class'], axis=1).iloc[idx].values).type('torch.FloatTensor')\n",
    "        #y = torch.from_numpy(self.df['Class'].iloc[idx]).type('torch.FloatTensor')\n",
    "        y = torch.tensor([self.df['Class'].iloc[idx]], dtype=torch.float32)\n",
    "        return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(seed, split_ratio) : \n",
    "    train_ratio = split_ratio[0]\n",
    "    valid_ratio = split_ratio[1]\n",
    "    test_ratio = split_ratio[2]\n",
    "    # normal\n",
    "    normal_data = df[df['Class']==0]\n",
    "    normal_data = normal_data.reset_index(drop=True)\n",
    "    # novelty\n",
    "    novel_data = df[df['Class']==1]\n",
    "    novel_data = novel_data.reset_index(drop=True)\n",
    "    \n",
    "    train_size = int(normal_data.shape[0]*train_ratio) # 60% train\n",
    "    valid_size = int(normal_data.shape[0]*valid_ratio) # 20% valid\n",
    "    test_size = int(normal_data.shape[0]*test_ratio) # 20% test\n",
    "  \n",
    "    # split train, valid, test\n",
    "    train_data = normal_data[:train_size]\n",
    "    valid_data = normal_data[train_size:train_size+valid_size]\n",
    "    test_data = normal_data[train_size+valid_size:]\n",
    "    \n",
    "    test_data = pd.concat([test_data, novel_data])\n",
    "    \n",
    "    train_dataset = CCFDataset(train_data)\n",
    "    valid_dataset = CCFDataset(valid_data)\n",
    "    test_dataset = CCFDataset(test_data)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input_dim=31 (except 'Time', 'Class'), hnum =1, z=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666.36328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wakanda/.local/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 Batch 0 loss : 2010.1145\n",
      "Train Epoch 0 Batch 50 loss : 1367.1299\n",
      "Train Epoch 0 Batch 100 loss : 788.3542\n",
      "Train Epoch 0 Batch 150 loss : 431.2951\n",
      "Train Epoch 0 Batch 200 loss : 283.1379\n",
      "Train Epoch 0 Batch 250 loss : 200.7375\n",
      "Train Epoch 0 Batch 300 loss : 137.4298\n",
      "Train Epoch 0 Batch 350 loss : 98.2401\n",
      "Train Epoch 0 Batch 400 loss : 69.8351\n",
      "Train Epoch 0 Batch 450 loss : 50.4254\n",
      "Train Epoch 0 Batch 500 loss : 39.0235\n",
      "Train Epoch 0 Batch 550 loss : 33.2224\n",
      "Train Epoch 0 Batch 600 loss : 29.5727\n",
      "Train Epoch 0 Batch 650 loss : 27.5031\n",
      "Valid Epoch 0 Batch 0 loss : 96.2912\n",
      "Valid Epoch 0 Batch 50 loss : 94.8732\n",
      "Valid Epoch 0 Batch 100 loss : 94.8177\n",
      "Valid Epoch 0 Batch 150 loss : 96.5335\n",
      "Valid Epoch 0 Batch 200 loss : 97.9995\n",
      "Test Epoch 0 Batch 0 loss : 95.1167\n",
      "Test Epoch 0 Batch 50 loss : 96.4028\n",
      "Test Epoch 0 Batch 100 loss : 96.5434\n",
      "Test Epoch 0 Batch 150 loss : 100.2075\n",
      "Test Epoch 0 Batch 200 loss : 96.7765\n",
      "Train Epoch 1 Batch 0 loss : 26.1105\n",
      "Train Epoch 1 Batch 50 loss : 24.3480\n",
      "Train Epoch 1 Batch 100 loss : 21.8754\n",
      "Train Epoch 1 Batch 150 loss : 20.0907\n",
      "Train Epoch 1 Batch 200 loss : 19.7887\n",
      "Train Epoch 1 Batch 250 loss : 19.2736\n",
      "Train Epoch 1 Batch 300 loss : 18.6969\n",
      "Train Epoch 1 Batch 350 loss : 18.0070\n",
      "Train Epoch 1 Batch 400 loss : 17.4273\n",
      "Train Epoch 1 Batch 450 loss : 17.0456\n",
      "Train Epoch 1 Batch 500 loss : 16.4657\n",
      "Train Epoch 1 Batch 550 loss : 17.2185\n",
      "Train Epoch 1 Batch 600 loss : 16.3736\n",
      "Train Epoch 1 Batch 650 loss : 16.1666\n",
      "Valid Epoch 1 Batch 0 loss : 35.2796\n",
      "Valid Epoch 1 Batch 50 loss : 35.8113\n",
      "Valid Epoch 1 Batch 100 loss : 37.3562\n",
      "Valid Epoch 1 Batch 150 loss : 36.3725\n",
      "Valid Epoch 1 Batch 200 loss : 36.2240\n",
      "Test Epoch 1 Batch 0 loss : 36.9226\n",
      "Test Epoch 1 Batch 50 loss : 36.8230\n",
      "Test Epoch 1 Batch 100 loss : 37.1261\n",
      "Test Epoch 1 Batch 150 loss : 34.1471\n",
      "Test Epoch 1 Batch 200 loss : 39.4823\n",
      "Train Epoch 2 Batch 0 loss : 15.0940\n",
      "Train Epoch 2 Batch 50 loss : 15.6647\n",
      "Train Epoch 2 Batch 100 loss : 15.3293\n",
      "Train Epoch 2 Batch 150 loss : 15.4895\n",
      "Train Epoch 2 Batch 200 loss : 16.4079\n",
      "Train Epoch 2 Batch 250 loss : 16.2561\n",
      "Train Epoch 2 Batch 300 loss : 14.2191\n",
      "Train Epoch 2 Batch 350 loss : 13.9252\n",
      "Train Epoch 2 Batch 400 loss : 14.5610\n",
      "Train Epoch 2 Batch 450 loss : 15.0671\n",
      "Train Epoch 2 Batch 500 loss : 13.4958\n",
      "Train Epoch 2 Batch 550 loss : 13.4662\n",
      "Train Epoch 2 Batch 600 loss : 14.9761\n",
      "Train Epoch 2 Batch 650 loss : 13.8808\n",
      "Valid Epoch 2 Batch 0 loss : 23.7626\n",
      "Valid Epoch 2 Batch 50 loss : 23.9679\n",
      "Valid Epoch 2 Batch 100 loss : 24.0109\n",
      "Valid Epoch 2 Batch 150 loss : 22.5456\n",
      "Valid Epoch 2 Batch 200 loss : 22.7881\n",
      "Test Epoch 2 Batch 0 loss : 25.6459\n",
      "Test Epoch 2 Batch 50 loss : 22.6771\n",
      "Test Epoch 2 Batch 100 loss : 22.6828\n",
      "Test Epoch 2 Batch 150 loss : 23.5257\n",
      "Test Epoch 2 Batch 200 loss : 28.1033\n",
      "Train Epoch 3 Batch 0 loss : 14.4721\n",
      "Train Epoch 3 Batch 50 loss : 13.3221\n",
      "Train Epoch 3 Batch 100 loss : 13.6337\n",
      "Train Epoch 3 Batch 150 loss : 13.8001\n",
      "Train Epoch 3 Batch 200 loss : 14.0020\n",
      "Train Epoch 3 Batch 250 loss : 14.4973\n",
      "Train Epoch 3 Batch 300 loss : 14.3533\n",
      "Train Epoch 3 Batch 350 loss : 14.6452\n",
      "Train Epoch 3 Batch 400 loss : 13.3101\n",
      "Train Epoch 3 Batch 450 loss : 13.8647\n",
      "Train Epoch 3 Batch 500 loss : 13.4796\n",
      "Train Epoch 3 Batch 550 loss : 12.6939\n",
      "Train Epoch 3 Batch 600 loss : 12.8845\n",
      "Train Epoch 3 Batch 650 loss : 13.3941\n",
      "Valid Epoch 3 Batch 0 loss : 17.8683\n",
      "Valid Epoch 3 Batch 50 loss : 19.6876\n",
      "Valid Epoch 3 Batch 100 loss : 19.6721\n",
      "Valid Epoch 3 Batch 150 loss : 19.5423\n",
      "Valid Epoch 3 Batch 200 loss : 18.9771\n",
      "Test Epoch 3 Batch 0 loss : 18.1369\n",
      "Test Epoch 3 Batch 50 loss : 17.5271\n",
      "Test Epoch 3 Batch 100 loss : 19.8560\n",
      "Test Epoch 3 Batch 150 loss : 20.7867\n",
      "Test Epoch 3 Batch 200 loss : 17.8765\n",
      "Train Epoch 4 Batch 0 loss : 15.2701\n",
      "Train Epoch 4 Batch 50 loss : 13.4269\n",
      "Train Epoch 4 Batch 100 loss : 13.8326\n",
      "Train Epoch 4 Batch 150 loss : 13.8992\n",
      "Train Epoch 4 Batch 200 loss : 15.1746\n",
      "Train Epoch 4 Batch 250 loss : 13.4428\n",
      "Train Epoch 4 Batch 300 loss : 14.2654\n",
      "Train Epoch 4 Batch 350 loss : 14.4485\n",
      "Train Epoch 4 Batch 400 loss : 13.7086\n",
      "Train Epoch 4 Batch 450 loss : 14.4226\n",
      "Train Epoch 4 Batch 500 loss : 13.9187\n",
      "Train Epoch 4 Batch 550 loss : 15.0166\n",
      "Train Epoch 4 Batch 600 loss : 12.7923\n",
      "Train Epoch 4 Batch 650 loss : 13.3673\n",
      "Valid Epoch 4 Batch 0 loss : 16.4901\n",
      "Valid Epoch 4 Batch 50 loss : 16.1325\n",
      "Valid Epoch 4 Batch 100 loss : 17.7891\n",
      "Valid Epoch 4 Batch 150 loss : 17.3966\n",
      "Valid Epoch 4 Batch 200 loss : 16.0201\n",
      "Test Epoch 4 Batch 0 loss : 18.7302\n",
      "Test Epoch 4 Batch 50 loss : 18.8420\n",
      "Test Epoch 4 Batch 100 loss : 16.0773\n",
      "Test Epoch 4 Batch 150 loss : 17.9564\n",
      "Test Epoch 4 Batch 200 loss : 16.4315\n",
      "Train Epoch 5 Batch 0 loss : 12.5046\n",
      "Train Epoch 5 Batch 50 loss : 14.2016\n",
      "Train Epoch 5 Batch 100 loss : 13.2686\n",
      "Train Epoch 5 Batch 150 loss : 13.3152\n",
      "Train Epoch 5 Batch 200 loss : 14.7298\n",
      "Train Epoch 5 Batch 250 loss : 12.5040\n",
      "Train Epoch 5 Batch 300 loss : 14.4135\n",
      "Train Epoch 5 Batch 350 loss : 13.3943\n",
      "Train Epoch 5 Batch 400 loss : 13.2411\n",
      "Train Epoch 5 Batch 450 loss : 12.1941\n",
      "Train Epoch 5 Batch 500 loss : 13.6724\n",
      "Train Epoch 5 Batch 550 loss : 13.1704\n",
      "Train Epoch 5 Batch 600 loss : 14.9750\n",
      "Train Epoch 5 Batch 650 loss : 12.9402\n",
      "Valid Epoch 5 Batch 0 loss : 14.8266\n",
      "Valid Epoch 5 Batch 50 loss : 15.5098\n",
      "Valid Epoch 5 Batch 100 loss : 15.2267\n",
      "Valid Epoch 5 Batch 150 loss : 15.6893\n",
      "Valid Epoch 5 Batch 200 loss : 16.6407\n",
      "Test Epoch 5 Batch 0 loss : 14.2845\n",
      "Test Epoch 5 Batch 50 loss : 16.4204\n",
      "Test Epoch 5 Batch 100 loss : 15.0282\n",
      "Test Epoch 5 Batch 150 loss : 15.7137\n",
      "Test Epoch 5 Batch 200 loss : 16.2999\n",
      "Train Epoch 6 Batch 0 loss : 13.7505\n",
      "Train Epoch 6 Batch 50 loss : 13.9247\n",
      "Train Epoch 6 Batch 100 loss : 14.8112\n",
      "Train Epoch 6 Batch 150 loss : 13.4938\n",
      "Train Epoch 6 Batch 200 loss : 12.9766\n",
      "Train Epoch 6 Batch 250 loss : 12.9284\n",
      "Train Epoch 6 Batch 300 loss : 12.8439\n",
      "Train Epoch 6 Batch 350 loss : 13.8160\n",
      "Train Epoch 6 Batch 400 loss : 13.4531\n",
      "Train Epoch 6 Batch 450 loss : 12.8356\n",
      "Train Epoch 6 Batch 500 loss : 13.5925\n",
      "Train Epoch 6 Batch 550 loss : 12.7097\n",
      "Train Epoch 6 Batch 600 loss : 13.1348\n",
      "Train Epoch 6 Batch 650 loss : 12.6893\n",
      "Valid Epoch 6 Batch 0 loss : 14.5801\n",
      "Valid Epoch 6 Batch 50 loss : 14.6807\n",
      "Valid Epoch 6 Batch 100 loss : 14.2834\n",
      "Valid Epoch 6 Batch 150 loss : 14.7161\n",
      "Valid Epoch 6 Batch 200 loss : 15.4755\n",
      "Test Epoch 6 Batch 0 loss : 15.9367\n",
      "Test Epoch 6 Batch 50 loss : 14.6075\n",
      "Test Epoch 6 Batch 100 loss : 16.2070\n",
      "Test Epoch 6 Batch 150 loss : 14.3762\n",
      "Test Epoch 6 Batch 200 loss : 15.8052\n",
      "Train Epoch 7 Batch 0 loss : 11.7168\n",
      "Train Epoch 7 Batch 50 loss : 13.9781\n",
      "Train Epoch 7 Batch 100 loss : 14.5799\n",
      "Train Epoch 7 Batch 150 loss : 13.4907\n",
      "Train Epoch 7 Batch 200 loss : 13.5482\n",
      "Train Epoch 7 Batch 250 loss : 14.2430\n",
      "Train Epoch 7 Batch 300 loss : 12.8955\n",
      "Train Epoch 7 Batch 350 loss : 12.0674\n",
      "Train Epoch 7 Batch 400 loss : 14.4197\n",
      "Train Epoch 7 Batch 450 loss : 12.9753\n",
      "Train Epoch 7 Batch 500 loss : 14.4765\n",
      "Train Epoch 7 Batch 550 loss : 12.9978\n",
      "Train Epoch 7 Batch 600 loss : 14.5257\n",
      "Train Epoch 7 Batch 650 loss : 13.0637\n",
      "Valid Epoch 7 Batch 0 loss : 13.7167\n",
      "Valid Epoch 7 Batch 50 loss : 14.5721\n",
      "Valid Epoch 7 Batch 100 loss : 14.6381\n",
      "Valid Epoch 7 Batch 150 loss : 15.0949\n",
      "Valid Epoch 7 Batch 200 loss : 14.3171\n",
      "Test Epoch 7 Batch 0 loss : 14.3983\n",
      "Test Epoch 7 Batch 50 loss : 13.5603\n",
      "Test Epoch 7 Batch 100 loss : 14.4660\n",
      "Test Epoch 7 Batch 150 loss : 14.1754\n",
      "Test Epoch 7 Batch 200 loss : 13.8649\n",
      "Train Epoch 8 Batch 0 loss : 14.2512\n",
      "Train Epoch 8 Batch 50 loss : 12.7836\n",
      "Train Epoch 8 Batch 100 loss : 12.3084\n",
      "Train Epoch 8 Batch 150 loss : 12.5370\n",
      "Train Epoch 8 Batch 200 loss : 13.2380\n",
      "Train Epoch 8 Batch 250 loss : 14.5769\n",
      "Train Epoch 8 Batch 300 loss : 15.0484\n",
      "Train Epoch 8 Batch 350 loss : 13.7395\n",
      "Train Epoch 8 Batch 400 loss : 13.9448\n",
      "Train Epoch 8 Batch 450 loss : 13.4309\n",
      "Train Epoch 8 Batch 500 loss : 13.3459\n",
      "Train Epoch 8 Batch 550 loss : 13.0141\n",
      "Train Epoch 8 Batch 600 loss : 13.5458\n",
      "Train Epoch 8 Batch 650 loss : 12.5469\n",
      "Valid Epoch 8 Batch 0 loss : 15.6776\n",
      "Valid Epoch 8 Batch 50 loss : 14.5305\n",
      "Valid Epoch 8 Batch 100 loss : 14.5154\n",
      "Valid Epoch 8 Batch 150 loss : 13.1749\n",
      "Valid Epoch 8 Batch 200 loss : 14.0957\n",
      "Test Epoch 8 Batch 0 loss : 15.8772\n",
      "Test Epoch 8 Batch 50 loss : 12.9279\n",
      "Test Epoch 8 Batch 100 loss : 13.2285\n",
      "Test Epoch 8 Batch 150 loss : 13.8583\n",
      "Test Epoch 8 Batch 200 loss : 15.3551\n",
      "Train Epoch 9 Batch 0 loss : 13.0730\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "zdims = 5\n",
    "h_num = 1\n",
    "epochs = 10\n",
    "split_ratio = [0.6, 0.2, 0.2]\n",
    "\n",
    "\n",
    "model = VAE(input_dim, zdims, h_num)\n",
    "train_loader, valid_loader, test_loader = split_data(seed, split_ratio)\n",
    "\n",
    "# train, validate the architecture\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "test_loss_list = []\n",
    "lowset_valid_loss = 0\n",
    "best_model = copy.deepcopy(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "print((len(train_loader.dataset) / batch_size))    \n",
    "\n",
    "for idx_epoch, epoch in enumerate(range(1, epochs+1)) :\n",
    "    # toggle train mode, intilaize loss for current epoch\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    test_loss = 0\n",
    "        \n",
    "    # training set \n",
    "    for idx_batch, (data, _) in enumerate(train_loader) :\n",
    "        # initilaize\n",
    "        optimizer.zero_grad()\n",
    "        # prediction\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        # get loss\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, batch_size, input_dim)\n",
    "        if idx_batch % 50 == 0 :\n",
    "            print('Train Epoch %d Batch %d loss : %.4f' % (idx_epoch, idx_batch, loss.item()))\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    train_loss = train_loss / (len(train_loader.dataset) / batch_size)\n",
    "    train_loss_list.append(train_loss)\n",
    "        \n",
    "    # validation set\n",
    "    model.eval()\n",
    "    for idx_batch, (data, _) in enumerate(valid_loader) :\n",
    "        with torch.no_grad() :\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar, batch_size, input_dim)\n",
    "            if idx_batch % 50 == 0 :\n",
    "                print('Valid Epoch %d Batch %d loss : %.4f' % (idx_epoch, idx_batch, loss.item()))\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "    valid_loss = valid_loss / (len(valid_loader.dataset) / batch_size)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    \n",
    "    # get best model\n",
    "    if epoch == 1 :\n",
    "        lowest_valid_loss = valid_loss\n",
    "    else : \n",
    "        if valid_loss <= lowest_valid_loss :\n",
    "            lowest_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "    \n",
    "    # test set\n",
    "    for idx_batch, (data, _) in enumerate(test_loader) :\n",
    "        with torch.no_grad() :\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar, batch_size, input_dim)\n",
    "            if idx_batch % 50 == 0 :\n",
    "                print('Test Epoch %d Batch %d loss : %.4f' % (idx_epoch, idx_batch, loss.item()))\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "    test_loss = test_loss / (len(test_loader.dataset) / batch_size)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "filepath = 'model1.mdl'\n",
    "torch.save(best_model.state_dict(), filepath)\n",
    "    \n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(valid_loss_list)\n",
    "plt.plot(test_loss_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = VAE(input_dim, zdims, h_num)\n",
    "#model.load_state_dict(torch.load(filepath))\n",
    "#model.eval()\n",
    "model = best_model\n",
    "\n",
    "for idx, (data, y) in enumerate(test_loader) :\n",
    "        if idx == 0 :\n",
    "            ytrue = y\n",
    "            with torch.no_grad() :\n",
    "                yhat = model(data)\n",
    "        else :\n",
    "            ytrue = torch.cat([ytrue, y])\n",
    "            with torch.no_grad() :\n",
    "                yhat = torch.cat([yhat, model(data)])\n",
    "                \n",
    "from sklearn.metrics import roc_auc_score\n",
    "auroc = roc_auc_score(ytrue, yhat)\n",
    "print('AUROC : ', roc_auc_score(ytrue, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input_dim=33, hnum =2, z=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
